========================================================
//                        KAFKA                       //
========================================================

#. Kafka получает поток байтов, не парсит их, не знает какой тип данных, не шифрует данные
- не загружает данные в память (это называется "zero copy")
- поэтому она быстрая

#. Kafka Connect
- в большинстве случаев данные, которые программист хочет положить (source) в кафку, берутся из типичных источков,
 так же и отправить (sink) из кафки програмист хочет в типичные места, например MongoDB etc.
- чтобы упростить разработку, уже есть написанные коннекторы для разных источников

#. Kafka Streams
- если нам нужно произвести действия в полученных из кафки данных, а потом отправить обратно в кафку в другой топик
- например, отфильтровать данные по условию
- если писать самому, будет много ошибок, особенно если прикрутить мультипоточность
- для этого есть готовое решение Kafka Streams API

#. Schema Registry
- так как кафка не знает ничего о данных (это сделано специально, чтобы она была быстрой),
- мы должни заботиться о том, чтобы данные приходили в одном формате, или же наш консюмер упадёт с ошибкой при парсинге
- чтобы получать валидный формат данных, есть Schema Registry, это отдельный компонент
- используется формат Apache Avro
- когда продюсер отправляет данные (в формате Apache Avro) в кафку, он отдельно отправляет схему формата данные в Schema Registry
- когда консюмер получает данные (в формате Apache Avro), он отдельно получает схему формата данные из Schema Registry
- нужно заменить код в продюсере и консюмере, потому что мы больше не можем использовать JSON, теперь только Apache Avro

========================================================
//                      STRUCTURE                     //
========================================================

#. Topics
- это набор партиций
- похоже на таблицу в БД, только без констрейнтов
- можно иметь сколько угодно типиков
- топики идентифицируются именем
- топики разбиты на партиции

#. Partitions
- именно в партицию попадаёт сообщение
- когда данные попали в партицию, их больше нельзя изменить
- собщения в пределах одной партиции имеют порядок
- offset - это id (порядковый номер) сообщения в партиции
- если слать в кафку сообщение с ключом, из ключа будет определен номер партиции, в которую ляжет сообщение,
 в таком случае, сообщения с одним ключом будут лежать в одной партиции, а значит будут упорядочены
- если поменять кол-во партиций, тогда старые сообщения останутся в старой партиции,
 а новые будут ложиться в новую, потому что вычисления партиции из ключа, зависит от общего кол-ва партиций в топике
- если слать сообщение без ключа, будет рандомно определено в какую партицию ляжет сообщение
- каждая партициия имеет 1 лидера и несколько ISR (in-sync replica), когла лидер отваливается, лидером становится ISR

#. Broker
- класер состоит из брокеров (серверов)
- брокер идентицифирован id
- каждый брокер содержит партиции конкретного топика, может содержать партиции из разных топиков
- после подключения к любому брокеру, вы будет поключены к целому кластеру
- хорошо иметь 3 брокерав кластере, но иногда кол-во может быть больше 100
- когда вы создаёте топик, его партиции будут автоматически распределены между брокерами

#. Broker Discovery
- каждый кафка брокер ещё называется "bootstrap server"
- каждый брокер знает всё про других брокеров, топики и партиции (метаданные)
- когда кафка подключается к одному из брокеров, она делает дополнительный запрос к этому брокеру, чтобы узнать
 список всех брокеров, чтобы при producing или consuming процессах автоматически знать к каким брокерам обращаться
- если в кластере хоть 100 брокеров, кафке нужно подключится только к одному, чтобы иметь доступ ко всему кластеру

#. Zookeeper
- кафка не может работать без zookeeper
- кафка управляет всеми метаданными в zookeeper
- кафка кластер подключается к zookeeper-кластеру
- разные брокеры подключены к разным zookeeper
- zookeeper менеджерит брокеры, хранит их список и другие данные (метаданные)
- zookeeper помогает в выборе лидера для партиций
- zookeeper посылает оповещения кафке при любых изменениях (упал брокер, новый топик, появился брокер, удалён топик и тд.)
- zookeeper спроектирован так, что работает с нечётным кол-вом серверов (3, 5, 7)
- zookeeper имеет лидер-сервер (обрабатывает запись от брокеров), остальные сервера называются followers (обрабатывает чтение)
- начиная от кафка > 0.10, zookeeper больше не хранит consumer offsets

#. Cluster
- это набор брокеров со своими топиками, партицииями и настройками

#. Topic replication factor
- кол-во копий партиции в других брокерах
- если какой-то брокер выйдет из строя, другой брокер, который имеет ISR реплику этой партиции,
 станет лидерои для партиции и продолжит работу
- в одно время только один брокер может быть лидером для конкретной партиции
- именно лидер получает и обслуживает данные конкретной партиции,
 другие служат пассивными репликами, которые синхронизируют данные
- каждая партициия имеет 1 лидера и несколько ISR (in-sync replica), когла лидер отваливается, лидером становится ISR
- на брокере могут быть так же реплики, которые не находятся в синхронизации (out of sync replicas)

#. Producer
- пишет данные в топик
- автоматически знает в какой брокер и в какую партицию писать
- если брокер упал, продюсер автоматически восстанавливается
- round robin - когда посылает сообщение без ключа, продюсер пишет по часть данных по очереди в каждый брокер
- может установить Acknowledgement (acks = 0/1/all)

#. Producer Acknowledgement
- 0 - не будет ждать подтверждения
- 1 - будет ждать подтверждение только от лидера
- all - будет ждать подтверждения от лидера и реплик

#. Consumer
- читает данные из топика
- автоматически знает из какого брокера читать
- если брокер упал, консюиер автоматически восстанавливается
- данные читаются упорядоченно в пределах одной партиции
- в джава коде, при создании консюмера, мы делаем subscribe на определённый topic,
 или вместо этого можно сделать assign на конкретную партицию конкретного топика
 (а потом получать (seek) сообщения начиная от указанного оффсета)

#. Consumer Groups
- каждый консюмер в группе консюмеров, читают данные из эксклюзивной партиции, это значит что в группе конюсюмер не
 читает данные из партиции, которую читает другой консюмер из его группы
- если у нас больше консюмеров, чем партиций, некоторые консюмеры будет неактивны

#. Consumer Offsets
- кафка хранит оффсеты, на которых консюмер группы читают сообщения
- они хранятся в топике с названием __consumer_offsets
- когда консюмер обработал данные, он должен закомиттить оффсеты,
 чтобы в следущий раз начать читать с того места, на котором закончил

#. Delivery semantic for Consumers
- at most once - сообщения комиттяться сразу после получения (данные могут потерятся)
- at least once - сообщения комиттятся только после обработки
 (может быть дупликация данных, если консюмер не будет idempotent)
- exactly once - можно достигнуть для кафка -> кафка процесса используя Kafka Streams API,
 а для кафка -> внешняя система, можно достигнуть используя idempotent consumer

#. Kafka Guarantees
- сообщения добавляются в топик-партицию в том порядке, в каком они туда отправлены
- консюмеры читают сообщения в том порядке, в котором они хранятся в топик-партиции
- с replication factor = n, продюсеры и консюмеры могут работать, если n-1 брокеры упадут,
 поэтому хорошая идея поставить replication factor = 3, позволяет выключить один брокер для поддержки,
 и ещё одному упасть непредвиденно, и при этом нормально работать дальше
- если кол-во партиций в топике неизменно, сообщения с одинаковым ключом будут ложиться в одну и ту же партицию

========================================================
//                       ADVANCED                     //
========================================================

#. Kafka Cluster Setup
- правильно настроить это не просто
- желательно изолировать каждый zookeeper + broker на отдельном сервере, чтобы иметь предсказуемые ресурсы
- нужно имплементировать мониторинг, видеть статистику, знать операции (Operations)
- надо быть хорошим кафка-админом

#. The most important kafka metrics
- Under Replicated Partitions: кол-во партиций, у которых есть проблемы с ISR, например они не синхронизированы
 (может значить, что система сильно нагружена)
- Request handlers: можно увидеть использование потоков для IO, network, overall kafka broker utilization
- Request timing: сколько времени занимает ответ на запрос (чем меньше, темм лучше); самое плохое сообщение должно
 дойти до консюмера меньше чем за 200 ms

#. Kafka Operations to know
- Rolling Restart of Brokers
- Updating Configurations
- Rebalancing Partitions
- Increasing Replication Factor
- Adding a Broker
- Replacing a Broker
- Removing a Broker
- Upgrading a Kafka Cluster with zero downtime

#. Security
- 9092 PLAINTEXT port -> no authentication
- 9093 SSL port -> ssl certificates
- SASL Authentication [PLAIN -> user/password (weak - easy to setup),
 Kerberos -> such as Microsoft Active Directory (strong - hard to setup),
 SCRAM -> username/password (strong -> medium to setup)]
- for authorization ACL (Access Control Lists) is used
- Bottom line: you can mix encryption/authentication/authorization

#. Kafka Multi Cluster + Replication
- Кафка может нормально рабоатать только в одном регионе, поэтому если ваше приложение используется в разных регионах,
 вам нужен отдельный кластер в каждом регионе
- если нужно получить данные из других регоинов, тогда используется репликация,
 по сути приложение репликаци - это просто продюсер + консюмер
- два подхода к репликации: active-passive, active-active
- при репликации оффсеты в разных кластерах не будут идентичны

========================================================
//                   RECOMMENDATIONS                  //
========================================================

#. пожелания по partitions number
- кол-во партиций должно быть 2x от кол-ва брокеров (< 6 brokers), 1х в (> 12 brokers)
- максимум 3x, в маленьких кластерах
- не создавайте 1 topic с 1000 партициями, чтобы достигнуть лучше пропускную способность (throughput)
- и не создавайте много брокеров, если у вас мало партиций

#. пожелания по replication factor
- должен быть как минимум 2, обычно 3, максимум 4
- когда replication factor = 3, должно быть как минимум 3 брокера
- никогда не ставьте replication factor = 1 в продакшне
- чем больше replication factor, тем больше задержка по времени, и кафка занимает больше места
- (на 50% больше места если replication factor = 3 по сравнениею с replication factor = 2)
- если replication factor создаёт проблемы производительности, не уменьшайте зачение,
 а возьмите более производительный комп

#. пожелания по brokers
- кол-во брокеров в любом кластере должно быть не меньше 3

#. пожелания по clusters
- кол-во партиций во всех топиках в одном брокере, не должно быть больше 2000-4000
- кол-во партиций во всех брокерах не должно быть больше 20000
- если вам нужно больше партиций (чем указано в пожеланиях), лучше добавьте ещё брокеров в кластер
- если вам нужно больше 20000 партиций в кластере, лучше сделайте несколько кластеров

========================================================
//                       PRODUCER                     //
========================================================

1. ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "true" // create safe producer
- включить идемпотентность
- producer в повторном запросе будет слать id, чтобы kafka не добавляла дубликатов

2. ProducerConfig.ACKS_CONFIG, "all"
- acks = all, значит что producer будет ждать, пока все реплики сохранят сообщение
- acks = all должно использоваться в сочетании с настройкой min.insync.replicas
- min.insync.replicas = 2 - минимум две реплики (включая лидера) должны дать ответ, что они сохранили сообщение
- можно выставить в настройках топика (при создании или изменить после) или брокера (в файле config/server.properties)
- на брокере могут быть так же реплики, которые не находятся в синхронизации (out of sync replicas)

3. ProducerConfig.RETRIES_CONFIG, Integer.toString(Integer.MAX_VALUE)
- кол-во попыток повторного отправления сообщения, если producer не получил ответа о успешном сохранении

4. ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, "5"
- кол-во параллельных запросов, при кафка > 1.1, порядок будет сохранён если значение > 1
- при предыдущих версиях, для сохранения порядка нужно ставить значение 1

5. ProducerConfig.COMPRESSION_TYPE_CONFIG, "snappy" // high throughput producer
(at the expense a bit of latency and CPU usage)
- будет сжимать batch указанным методом (snappy оптимально для текстовых данных)
- чем больше данных, тем больше данных сжимается, тем лучше результат

6. ProducerConfig.LINGER_MS_CONFIG, "1000"
- задержка перед тем как batch будет отправлен
- чем больше сообщений придёт через вызов метода producer.send() за указанное время,
тем больше данных уйдёт одним запросом
- если batch переполнен, он будет немедленно отправлен
- если сообщение больше, чем размер batch'а, оно не будет добавлено в batch, а отправится отдельным запросом
- by default is 0

7. ProducerConfig.BATCH_SIZE_CONFIG, Integer.toString(32 * 1024)
- размер batch
- by default is 16KB

8. ProducerConfig.MAX_BLOCK_MS_CONFIG, "60000"
- когда broker не может принимать данные, или producer шлёт больше, чем broker успеть обработать,
 тогда данные ложатся в буффер на стороне producer (BUFFER_MEMORY_CONFIG), а метод producer.send()
 блокируется на MAX_BLOCK_MS_CONFIG
- если broker не успевает всё обработать (или он упал), будет брошено исключение,
 но только по истечению MAX_BLOCK_MS_CONFIG
- by default is 60000

9. ProducerConfig.BUFFER_MEMORY_CONFIG, Integer.toString(32 * 1024 * 1024)
- размер буффера на стороне producer
- by default is 32MB

========================================================
//              PRODUCER KEYS HASH FORMULA            //
========================================================

- по дефолту используется murmur2 алгоритм (не рекоммендуется менять)
- targetPartition = Utils.abs(Utils.murmur2(record.key())) % numPartitions;
- партиция, в которую попадёт сообщение завитст от кол-ва партиций в топике
- можно изменить поведение в partitioner.class

========================================================
//                        OFFSETS                    //
========================================================

1. At most once
- когда консюмер получил batch сообщений, он закомитил оффсеты сразу после получения
- если впоследствии при дальнейшей обработки сообщений консюмером, он упадёт,
 когда консюмер продолжит читать, он начнёт с закомиченого оффсета,
 и те сообщения, которые были закомичены, но не обработаны, потеряются

2. At least once (by default)
- оффсеты комитятся только после обработки
- если консюмер упадёт, то продолжит с закомиченного оффсета,
 значит могут быть дубликаты, потому что некоторые сообщения уже были обработаны,
 но будут обработаны ещё один раз, потому что оффсет не был закомичен
- by default

3. Exactly once
- работает только для Kafka -> Kafka workflows using Kafka Streams API

========================================================
//               CONSUMER POLL BEHAVIOUR              //
========================================================

#. консюмер сам поллит данные из брокера,
- это сделано для того, чтобы можно было добавлять настройки

1. Fetch.min.bytes
- Fetch.min.bytes это минимальное кол-во байт, которые мы хотим получить за один полл реквест консюмера
- by default is 1 byte

2. Max.poll.records
- максимальное кол-во сообщений за один полл реквест
- by default is 500

3. Max.partitions.fetch.bytes
- максимальное кол-во данных возвращенных брокером для одной партиции
- by default is 1MB

4. Fetch.max.bytes
- максимальное кол-во байт, которые мы хотим получить за один fetch реквест консюмера
- (для множества партиций, не для одной)
- консюмер делает multiple fetch request in parallel
- by default is 50MB

========================================================
//         CONSUMER OFFSET COMMITS STRATEGIES         //
========================================================

1. (easy) enable.auto.commit = true & synchronous processing of batches
- мы не делает следующий полл, пока не обработали предыдущий
- enable.auto.commit by default is true

2. (medium) enable.auto.commit = false & manual commit of offset
- комиттит оффсеты вручную
- можем аккумулировать batch (результат .poll()) и потом по условию
- делать синхронный запрос и комиттить оффсет (consumer.commitSync())

#. с включённым авто комиттом, оффсеты будут комититься автоматически
- регулярно через интервал (auto.commit.interval.ms = 5000 by default)
- каждый раз когда мы вызываем .poll()

========================================================
//          CONSUMER OFFSET RESET STRATEGIES          //
========================================================

#. если консюмер не читает новые данные, то через определенное время (offset.retention.minutes) оффсеты станут невалидными,
 а когда консюмер заработает, он начнёт читать данные в зависимости от настройки auto.offset.reset

1. auto.offset.reset:
- latest - will read from the end of the log
- earliest - will read from the start of the log
- none - will throw an exception if no offset is found

2. offset.retention.minutes
- сколько времени должно пройти, чтобы оффсеты стали невалидными
- by default in kafka < 2.0 = 1 day
- by default in kafka >= 2.0 = 7 days

3. чтобы повторить данные для группы консюмеров:
- остановить все консюмеры из данной группы
- использовать комманду "kafka-consumer-group", чтобы установить оффсет который мы хотим
$ kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group my-first-application --reset-offsets --to-earliest --execute --topic twitter_topic
- перезапустить консюмеры

3. to replay data for a consumer group
- take all the consumers from a specific group down
- use "kafka-consumer-group" command to set offset to what you want
$ kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group my-first-application --reset-offsets --to-earliest --execute --topic twitter_topic
- restart consumers

========================================================
//           CONTROLLING CONSUMER LIVELINESS          //
========================================================

#. каждый косюмер в группе фетчит данные от брокера в Poll Thread,
- но также посылает heartbeat запросы в Heartbeat Thread, чтобы показать, что консюмер живой
- когда консюмер отваливается, происходит перебалансировка (какой консюмер из какой партиции читает)

1. session.timeout.ms
- heartbeats посылаются периодически брокеру
- если за этот период не было послано ни одного heartbeat, консюмер считается мёртвым и происходит перебалансировка
- если нужно, чтобы перебалансировка происходила быстрее, можно поставить меньшее значение
- by default is 10 seconds

2. heartbeat.interval.ms
- как часто посылать heartbeat запросы
- обычно выставляется как 1/3 от session.timeout.ms
- by default is 3 seconds

========================================================
//                 SEGMENT AND INDEXES                //
========================================================

#. Partition are made of Segments
- segment - это файл (log), который хранит определнное кол-во данных (range of offsets)
- только один segment может быть Active (тот в который кафка пишет сейчас)
- когда подходит лимит времени или размера для сегмента, он закрывается (комиттится) и открывается новый
- для каждого сегмента (лог файла), есть по файлу индексу (оффсет позиции, и таймстамп оффсета)
- эти файлы можно посмотреть в папках топиков

#. Segments come with two indexes
- offset to position index, по которому кафка может найти конкретное сообщение в сегменте (в log)
- timestamp to offset index, по которому кафка может найти сообщения за конкретную дату

#. log.segment.bytes
- максимальный размер сегмента в байтах
- by default is 1 GB
- если поставить значение меньше дефолтного, это приведёт к:
 + будет больше сегментов на партициию (больше файлов)
 + Log Compaction будет происходить чаще
 + кафка должна будет держать открытыми больше файлов (это может вызвать ошибку)

#. log.segment.ms
- время, которое кафка будет ждать перед тем как закрыть (commit) сегмент, если он не достиг лимит по размеру
- by default is 1 week
- если поставить значение меньше дефолтного, это приведёт к:
 + Log Compaction будет происходить чаще (каждый период, который выставлен в log.segment.ms)
 + будет больше сегментов на партициию (больше файлов)

========================================================
//                LOG CLEANUP POLICIES                //
========================================================

#. Кластеры удаляют устаревшие данные по определённой политике (log cleanup)

#. log.cleanup.policy = delete (by default for all user topics)
- удаление происходит на основании:
 + возраста данных (by default is 1 week)
 + размера лога (by default -1 == infinite)

#. log.cleanup.policy = compact (by default for __consumer_offsets topics)
- удаление происходит на основании  ключей данных
- удалит старые дупликаты ключей ПОСЛЕ того как активный сегмент будет закомичен (закрыт)
- создаст новый сегмент вместо старых (со всеми оффсетами старых сегментов), при этом активный сегмент остаётся нетронутым
- эта политика позволяет:
 + контролировать размер данных на диске (удалят устаревшие данные)
 + ограничить кол-во работы для поддержки кластера

#. LOG CLEANUP
- log cleanup up происходит с сегментами партиций
- чем меньше размер сегментов (а значит их кол-во больше), тем чаще будет происходить log cleanup
- log cleanup не должен происходит слишком часто (CPU and RAM resources usage)
- cleaner проверяет есть ли для него работа каждые 15 seconds (log.cleaner.backoff.ms настройка)
- log.cleaner.backoff.ms - интервал, через который cleaner проверяет есть ли для него работа

#. Log Cleanup Policy: Delete
- log.retention.hours - сколько часов кафка будет хранить данные (by default is 168 (1 week))
 + если поставить значение больше дефолтного, это приведёт к большему использованию места на диске
 + если поставить значение меньше дефолтного, то меньше данных будет храниться (если консюмер будет долго лежать,
  то когда он восстановится, есть возможность, что данные, которые он не успел прочитать уже будут удалены)
- log.retention.bytes - максимальный размер лога в байтах для каждой партиции (by default -1 == infinite)
 + бывает полезно контролировать размер лога по какому-то пределу (threshold)
- две общие пары настроек:
 + one week of retention (сохранения) -> log.retention.hours = 168 and log.retention.bytes = -1
 + infinite time retention  bounded by 500MB -> log.retention.hours = 17520 and log.retention.bytes = 524288000

#. Log Cleanup Policy: Compact
- гарантирует, что лог будет содержать как минимум только самые новые известные значения для конкретных
 ключей в пределах одной партиции
- это полезно, если нам нужен SNAPSHOT, а не полная история сообщений (как в таблице БД)
- идея в том, что мы храним только последние обновления значений для ключей в нашем логе
- after compaction a new segment will be created (без старых данных)
- свойства:
 + любой консюмер, который читает с хвоста лога (самые новые данные) будет видеть все сообщения, которые приходят в топик
 + порядок сообщений сохраняется, log compaction только удаляет некоторые сообщения, но не переупорядочивает их
 + оффсет сообщений неизменяемый, офссеты просто пропускаются, если сообщения там нет сообщений
 + удалённые сообщения могут быть всё ещё видны консюмерам на определённый период (delete.retention.ms настройка)
- delete.retention.ms - период, в который удалённые собщения остаются видны консюмерам (by default is 24 hours)

#. Log compaction
- не защищает от добавления дубликатов в кафку
 + дедупликация происходит после того как сегмент закомичен
 + консюмеры всё ещё будут видеть все сообщения с хвоста по мере их поступления
- не защищает от чтения дубликатов из кафки
- время от времени log compaction может проваливаться (поток, в котором он испольняется может падать)
 + удостоверьте, что у вас достаточно памяти
 + перезапустите кафку, если log compaction упал
- нельзя самому запустить log compaction через API (может в будущем такая возможность появится)

#. Log compaction настраивается настройкой log.cleanup.policy = compact
 + segments.ms (default is 7 days) - сколько ждать перед тем как закрыть (закомитить) активный сегмент
 + segments.bytes (default is 1 GB) - максимальный размер сегмента
 + min.compaction.lag.ms (default is 0) - сколько ждать перед тем как сообщение может быть compacted
 + delete.retention.ms (default is 24 hours) - сколько времени консюмер всё ещё может видеть удалённые сообщения,
  перед тем, как они будут полностью удалены
 + min.cleanable.dirty.ratio (default is 0.5) - чем больше зачение, тем меньше compaction (более эффективная чистка),
  0.5 - это хорошее соотношение

========================================================
//               UNCLEAN LEADER ELECTIONS             //
========================================================

#. unclean.leader.election
- если все ISR (in sync replicas) упадут, но у нас есть работающие out of sync replicas, у нас есть следующие варианты:
 + или мы ждём когда заработает ISR
 + или ставим unclean.leader.election = true и начинаем писать данные в НЕ ISR партиции
- если это сделать, мы получим availability, но потеряем consistency, потому что данные в ISR будут потеряны
- это может быть использовано только там, где потеря данных допустима (логи, метрика)

========================================================
//               ADVERTISED HOST SETTING              //
========================================================

#. когда есть кафка-клиент и кафка-брокер, у кафка-брокера есть: public IP, private IP, ADV_HOST
 (advertised host or advertised listener)
- когда клиент подключает к кафке по IP, кафка отвечает, что получила запрос, и говорит клиенту,
 что он должен использовать advertised hostname (IP) и отправляет его значение клиенту
- клиент подключается по advertised hostname:
 + если клиент и кафка находятся в одной сети, то проблем не будет
 + если клиент и кафка находятся в разных сетях, то клиент получит ошибку в запросе к advertised hostname
- при испольщовании localhost как advertised hostname (по умолчанию при локальной разработке), то:
 + это будет работать, если и клиент и кафка находятся на одной машине, и у нас только 1 брокер
 + не будет работать, если и клиент и кафка не находятся на одной машине, или если у нас 2 и больше брокеров
- если перезапустить кафку, её public IP изменится, и клиент не сможет достучаться к кафке

#. advertised.listeners (настройка в config/server.properties)
- если клиент приватной сети (там же где и кафка):
 + ставим в advertised.listeners internal private IP или internal private DNS hostname
  (клиенты должны иметь возможность достучаться к внутреннему private IP или hostname)
- если клиент в публичной сети (не там же где и кафка):
 + ставим в advertised.listeners external public IP или external public DNS hostname pointing to the public IP
  (клиенты должны иметь возможность достучаться к public DNS)

#. когда правим advertised.listeners настройку в config/server.properties
- раскомментируем строку с advertised.listeners и добавляем правильный IP и порт (9092)
- PLAINTEXT:// оставлям, то есть вид значения такой: PLAINTEXT://IP:PORT
- не перепутайте с настройкой listeners

-----------------------------------------------------------------------------------

